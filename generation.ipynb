{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60014fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd \n",
    "import re\n",
    "\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain import hub\n",
    "from typing_extensions import List, TypedDict, Optional\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langchain_core.documents import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e1ece89",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fireworksai_api_key.txt', 'r') as file:\n",
    "    API_KEY = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76d690d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "vectorstore = Qdrant.from_existing_collection(\n",
    "    collection_name=\"planten\",\n",
    "    embedding=embeddings,\n",
    "    path=\"vector_stores/plantkiezer1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f041217",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/texas_plant_list_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0114ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which plants are nice in a humid environment?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b9e8e1",
   "metadata": {},
   "source": [
    "# Query Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "504533d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_expansion_instruct = \"You enrich a user query for dense vector search. Return ONE line: the original query first, then up to 5 short synonym/keyword variants separated by ' | '. Preserve intent; prefer domain-specific terms likely found in the corpus. Each variant 2-6 words. No quotes, no explanations, no boolean operators, nothing else.\"\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma3:4b\", \n",
    "    keep_alive=\"30m\",\n",
    "    num_ctx=2048,\n",
    "    num_predict=256,\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        query_expansion_instruct,\n",
    "    ),\n",
    "    (\n",
    "        \"human\", \n",
    "        query),\n",
    "]\n",
    "\n",
    "query_expanded = llm.invoke(messages).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49145b37",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62f60874",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\n",
    "    # \"accounts/fireworks/models/deepseek-v3\",\n",
    "    # \"accounts/fireworks/models/qwen3-30b-a3b-instruct-2507\", \n",
    "    # \"accounts/fireworks/models/gpt-oss-20b\",\n",
    "    # \"accounts/fireworks/models/gpt-oss-120b\",\n",
    "    \"accounts/fireworks/models/llama-v3p1-405b-instruct\",\n",
    "    model_provider=\"fireworks\", \n",
    "    api_key=API_KEY\n",
    ")\n",
    "\n",
    "# MODEL = \"gpt-oss-20b\"\n",
    "# MODEL = \"gpt-oss-120b\"\n",
    "# MODEL = \"qwen3-30b\"\n",
    "# MODEL = \"deepseek-v3\"\n",
    "MODEL = \"llama-v3p1-405b-instruct\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec6788b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "    ids: List[int]\n",
    "    \n",
    "    # generation & retrieval controls\n",
    "    max_tokens: Optional[int]\n",
    "    top_p: Optional[float]\n",
    "    top_k: Optional[int]\n",
    "    presence_penalty: Optional[float]\n",
    "    frequency_penalty: Optional[float]\n",
    "    temperature: Optional[float]\n",
    "\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = list(vectorstore.max_marginal_relevance_search(query_expanded, k=3, filter=None))\n",
    "\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "\n",
    "    ids = re.findall(r\"ID:\\s*(\\d+)\\s*\\|\", docs_content)\n",
    "    ids = [int(i) for i in ids]\n",
    "\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    return {\"answer\": response.content, \"ids\": ids}\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b8c8443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruction = \"You are an expert botanical assistant. You will be provided with five retrieved plant entries. Choose three of them that you think answers the user query the best and recommend it. Use the descriptions of the retrieved data to also provide more information about the plants. Then after your response to the user, write the IDs of the three plants you recommended in the format 'Recommended plant IDs: ID1, ID2, ID3'.\"\n",
    "\n",
    "instruction = \"You are an expert botanical assistant and also a sales chatbot. You will be provided with three retrieved plant entries. Answer the user query by recommending these three plants. Use the descriptions of the retrieved data to also provide more information about the plants. Frame your response concisely, while also like a real salesperson. Here is the user question: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30a552e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = instruction + query\n",
    "\n",
    "response = graph.invoke({\n",
    "        \"question\": query,\n",
    "        \"max_tokens\": 1024,\n",
    "        \"top_p\": 1,\n",
    "        \"top_k\": 40,\n",
    "        \"presence_penalty\": 0,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"temperature\": 0.6,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01d12148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[279, 413, 400]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']\n",
    "\n",
    "ids = [id - 1 for id in response['ids']]\n",
    "ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
